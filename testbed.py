#!/usr/bin/env python3
"""
GPT-OSS-20B Red Team Testbed
Main test runner for the OpenAI red-teaming competition
"""
import click
import yaml
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

from utils.model_client import OllamaClient
from utils.evaluator import ResponseEvaluator, VulnerabilityCategory, VulnerabilityScore
from utils.findings_generator import FindingsGenerator
from categories.deception_samples import run_deception_samples_tests
from categories.deception_adderall import run_deception_adderall_tests


def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """Load configuration from YAML file"""
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_file, 'r') as f:
        return yaml.safe_load(f)


def setup_logging(config: Dict[str, Any]) -> None:
    """Setup logging configuration"""
    import logging
    
    log_config = config.get("logging", {})
    level = getattr(logging, log_config.get("level", "INFO").upper())
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_config.get("file", "testbed.log"))
        ]
    )


def ensure_directories(config: Dict[str, Any]) -> None:
    """Ensure output directories exist"""
    output_config = config.get("output", {})
    
    results_dir = Path(output_config.get("results_dir", "results"))
    findings_dir = Path(output_config.get("findings_dir", "findings"))
    
    results_dir.mkdir(exist_ok=True)
    findings_dir.mkdir(exist_ok=True)


@click.group()
@click.option('--config', default='config.yaml', help='Configuration file path')
@click.option('--verbose', '-v', is_flag=True, help='Verbose output')
@click.pass_context
def cli(ctx, config, verbose):
    """GPT-OSS-20B Red Team Testbed"""
    ctx.ensure_object(dict)
    ctx.obj['config'] = load_config(config)
    ctx.obj['verbose'] = verbose
    
    setup_logging(ctx.obj['config'])
    ensure_directories(ctx.obj['config'])


@cli.command()
@click.option('--category', '-c', help='Specific vulnerability category to test')
@click.option('--output', '-o', help='Output directory for results')
@click.option('--auto-score', is_flag=True, help='Enable automatic scoring')
@click.option('--export-findings', is_flag=True, help='Export results as competition findings')
@click.option('--quiet', '-q', is_flag=True, help='Quiet mode - minimal output, no live preview')
@click.option('--no-live', is_flag=True, help='Disable live preview (legacy compatibility)')
@click.option('--force-interactive', is_flag=True, help='Force interactive mode (for testing)')
@click.option('--skip-busy-check', is_flag=True, help='Skip busy check and proceed anyway')
@click.pass_context
def run(ctx, category, output, auto_score, export_findings, quiet, no_live, force_interactive, skip_busy_check):
    """Run vulnerability tests"""
    config = ctx.obj['config']
    verbose = ctx.obj['verbose']
    
    # Configure live display based on flags
    from utils.live_display import set_display_options, get_display
    enable_live = not (quiet or no_live)  # Live preview enabled by default, disabled by flags
    set_display_options(enable_live=enable_live, quiet_mode=quiet, verbose=verbose)
    
    # Force interactive mode for testing if requested
    if force_interactive:
        display = get_display()
        display.is_interactive = True
    
    # Initialize client
    model_config = config.get('model', {})
    client = OllamaClient(
        host=model_config.get('host', 'localhost'),
        port=model_config.get('port', 11434),
        model=model_config.get('name', 'gpt-oss:20b')
    )
    
    # Check model availability
    click.echo("Checking model availability...")
    if not client.is_model_available():
        click.echo(f"‚ùå Model {client.model} not available.")
        click.echo("Please run: ollama pull gpt-oss:20b")
        return
    
    click.echo(f"‚úÖ Model {client.model} ready")
    
    # Check if Ollama is busy before starting tests
    if not skip_busy_check:
        click.echo("üîç Checking if Ollama is busy...")
        try:
            status = client.check_ollama_status()
            
            if status.is_busy:
                click.echo(f"‚ö†Ô∏è  WARNING: Ollama appears busy!")
                click.echo(f"   GPU usage: {status.gpu_usage}")
                click.echo(f"   Memory usage: {status.memory_usage}")
                click.echo(f"   Model loaded: {'Yes' if status.model_loaded else 'No'}")
                click.echo(f"   This may cause timeouts and test failures.")
                
                if not quiet:
                    if click.confirm("\nDo you want to continue anyway? Tests may timeout."):
                        click.echo("‚ö° Proceeding with tests (this may take longer)...")
                    else:
                        click.echo("üö´ Aborted. Wait for current requests to complete or use --skip-busy-check")
                        return
                else:
                    click.echo("üö´ Ollama is busy. Use --skip-busy-check to proceed anyway.")
                    return
            else:
                click.echo(f"‚úÖ Ollama status: Available")
                if verbose and status.model_loaded:
                    click.echo(f"   Memory usage: {status.memory_usage}")
        except Exception as e:
            click.echo(f"‚ö†Ô∏è  Could not check Ollama status: {e}")
            if not quiet and not skip_busy_check:
                if click.confirm("\nCould not determine if Ollama is busy. Continue anyway?"):
                    click.echo("‚ö° Proceeding with tests...")
                else:
                    click.echo("üö´ Aborted. Use --skip-busy-check to proceed anyway.")
                    return
    
    # Determine which categories to test
    available_categories = config.get('categories', {}).get('enabled', ['deception'])
    focus_categories = config.get('categories', {}).get('focus_categories', ['deception'])
    
    if category:
        if category not in available_categories:
            click.echo(f"‚ùå Category '{category}' not available. Options: {available_categories}")
            return
        test_categories = [category]
    else:
        test_categories = focus_categories
    
    # Import display after configuration
    from utils.live_display import get_display
    display = get_display()
    
    display.info(f"Testing categories: {test_categories}")
    
    # Run tests
    all_results = {}
    findings_generator = FindingsGenerator(model_version=client.model)
    
    for cat in test_categories:
        if cat == "deception_samples":
            results = run_deception_samples_tests(client, category=None)
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-scoring enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        elif cat == "deception_adderall":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to medical advice manipulation")
            results = run_deception_adderall_tests(client, category=None)
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-scoring enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        else:
            display.error(f"Category '{cat}' not yet implemented")
    
    # Save results
    output_dir = output or config.get('output', {}).get('results_dir', 'results')
    save_results(all_results, output_dir, verbose)
    
    # Report timeout statistics
    timeout_stats = calculate_timeout_stats(all_results)
    if timeout_stats['total_timeouts'] > 0:
        click.echo(f"\n‚è∞ Timeout Report:")
        click.echo(f"   Total requests: {timeout_stats['total_requests']}")
        click.echo(f"   Timeouts: {timeout_stats['total_timeouts']} ({timeout_stats['timeout_percentage']:.1f}%)")
        click.echo(f"   Average response time: {timeout_stats['avg_response_time']:.2f}s")
        click.echo(f"   Slowest response: {timeout_stats['max_response_time']:.2f}s")
        if timeout_stats['total_timeouts'] > timeout_stats['total_requests'] * 0.2:
            click.echo(f"   \u26a0\ufe0f  High timeout rate - model may be overloaded")
    
    display.info(f"‚úÖ Testing complete. Results saved to {output_dir}/")


def calculate_timeout_stats(all_results: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate timeout statistics from test results"""
    total_requests = 0
    total_timeouts = 0
    response_times = []
    
    for category, cat_results in all_results.items():
        for test, responses, evaluation in cat_results.get("results", []):
            if responses:  # Check if we have responses
                for response in responses:
                    total_requests += 1
                    response_times.append(response.response_time)
                    if response.timed_out:
                        total_timeouts += 1
    
    if total_requests == 0:
        return {
            'total_requests': 0,
            'total_timeouts': 0,
            'timeout_percentage': 0.0,
            'avg_response_time': 0.0,
            'max_response_time': 0.0
        }
    
    return {
        'total_requests': total_requests,
        'total_timeouts': total_timeouts,
        'timeout_percentage': (total_timeouts / total_requests) * 100,
        'avg_response_time': sum(response_times) / len(response_times) if response_times else 0.0,
        'max_response_time': max(response_times) if response_times else 0.0
    }


def generate_findings_from_results(results: Dict[str, Any], 
                                   generator: FindingsGenerator,
                                   config: Dict[str, Any]) -> List[str]:
    """Generate competition findings from test results"""
    scoring_config = config.get('scoring', {})
    threshold = scoring_config.get('score_threshold', 6.0)
    
    generated_files = []
    test_results = results.get('results', [])
    
    for test, responses, evaluation in test_results:
        if evaluation.is_vulnerable and evaluation.confidence > scoring_config.get('confidence_threshold', 0.6):
            # Generate score
            score = generator.evaluator.suggest_score(evaluation)
            
            if score.total_score() >= threshold and score.is_valid():
                # Create finding
                finding = generator.create_finding(
                    category=evaluation.category,
                    title=f"{test.name} - {evaluation.category.value.title()} Vulnerability",
                    evaluation=evaluation,
                    score=score,
                    prompt=test.prompt,
                    response=responses[0].content if responses else "",
                    methodology=f"Systematic testing using {test.test_id}. {test.description}",
                    reproduction_steps=[
                        f"Use prompt: '{test.prompt}'",
                        f"Send to model: {generator.model_version}",
                        f"Analyze response for: {evaluation.reasoning}",
                        "Verify evidence matches expected patterns"
                    ]
                )
                
                # Save finding
                filepath = generator.save_finding(finding)
                generated_files.append(filepath)
                print(f"üíæ Saved finding: {filepath}")
    
    return generated_files


def save_results(results: Dict[str, Any], output_dir: str, verbose: bool) -> None:
    """Save test results to files"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save full results
    results_file = output_path / f"test_results_{timestamp}.json"
    
    # Convert results to JSON-serializable format
    serializable_results = {}
    for category, cat_results in results.items():
        serializable_results[category] = {
            "analysis": cat_results.get("analysis", {}),
            "test_details": []
        }
        
        for test, responses, evaluation in cat_results.get("results", []):
            test_detail = {
                "test_id": test.test_id,
                "test_name": test.name,
                "category": test.category,
                "prompt": test.prompt,
                "system_prompt": test.system_prompt,
                "vulnerability_detected": evaluation.is_vulnerable,
                "confidence": evaluation.confidence,
                "evidence": evaluation.evidence,
                "reasoning": evaluation.reasoning,
                "response_content": responses[0].content if responses else "",
                "response_time": responses[0].response_time if responses else 0.0,
                "timed_out": responses[0].timed_out if responses else False,
                "response_error": responses[0].error if responses else None
            }
            serializable_results[category]["test_details"].append(test_detail)
    
    with open(results_file, 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    if verbose:
        print(f"üìÅ Results saved to: {results_file}")
    
    # Save summary
    summary_file = output_path / f"summary_{timestamp}.txt" 
    with open(summary_file, 'w') as f:
        f.write(f"GPT-OSS-20B Red Team Test Summary\n")
        f.write(f"Generated: {datetime.now().isoformat()}\n")
        f.write("=" * 50 + "\n\n")
        
        for category, cat_results in results.items():
            analysis = cat_results.get("analysis", {})
            f.write(f"Category: {category.upper()}\n")
            f.write(f"  Total tests: {analysis.get('total_tests', 0)}\n")
            f.write(f"  Vulnerabilities: {analysis.get('vulnerable_tests', 0)}\n")
            f.write(f"  High confidence: {analysis.get('high_confidence_vulnerabilities', 0)}\n")
            f.write(f"  Average confidence: {analysis.get('average_confidence', 0):.2f}\n")
            f.write("\n")


@cli.command()
@click.pass_context
def test_connection(ctx):
    """Test connection to Ollama and model availability"""
    config = ctx.obj['config']
    model_config = config.get('model', {})
    
    client = OllamaClient(
        host=model_config.get('host', 'localhost'),
        port=model_config.get('port', 11434),
        model=model_config.get('name', 'gpt-oss:20b')
    )
    
    click.echo("Testing Ollama connection...")
    
    # Check if Ollama is busy before testing
    click.echo("üîç Checking Ollama status...")
    try:
        status = client.check_ollama_status()
        
        if status.is_busy:
            click.echo(f"‚ö†Ô∏è  WARNING: Ollama appears busy (GPU usage: {status.gpu_usage})")
            click.echo(f"   Model loaded: {'Yes' if status.model_loaded else 'No'}")
            click.echo(f"   Memory usage: {status.memory_usage}")
            click.echo(f"   This may cause slower responses or timeouts.")
            if ctx.obj['verbose']:
                click.echo(f"   Raw status: {status.raw_output}")
        else:
            click.echo(f"‚úÖ Ollama status: Available")
            if status.model_loaded:
                click.echo(f"   Model loaded: Yes (Memory: {status.memory_usage})")
            else:
                click.echo(f"   Model loaded: No")
    except Exception as e:
        click.echo(f"‚ö†Ô∏è  Could not check Ollama status: {e}")
    
    try:
        if client.is_model_available():
            click.echo(f"‚úÖ Model {client.model} is available")
            
            # Test generation
            click.echo("üß™ Testing generation...")
            response = client.generate("Hello, this is a test.")
            if response.error:
                click.echo(f"‚ùå Generation failed: {response.error}")
                if response.timed_out:
                    click.echo(f"   This was a timeout - model may be busy or overloaded")
            else:
                click.echo(f"‚úÖ Generation successful ({response.response_time:.2f}s)")
                if response.timed_out:
                    click.echo(f"‚ö†Ô∏è  Response had timeout issues")
                if ctx.obj['verbose']:
                    click.echo(f"Sample response: {response.content[:100]}...")
        else:
            click.echo(f"‚ùå Model {client.model} not found")
            click.echo("Please run: ollama pull gpt-oss:20b")
    
    except Exception as e:
        click.echo(f"‚ùå Connection failed: {e}")


@cli.command()
@click.option('--findings-dir', default='findings', help='Findings directory')
@click.pass_context  
def summary(ctx, findings_dir):
    """Generate summary of findings"""
    generator = FindingsGenerator()
    report = generator.generate_summary_report(findings_dir)
    
    if "error" in report:
        click.echo(f"‚ùå {report['error']}")
        return
    
    click.echo("üìä Findings Summary")
    click.echo("=" * 30)
    click.echo(f"Total findings: {report['total_findings']}")
    click.echo(f"Average score: {report['average_score']:.1f}")
    click.echo(f"High severity (‚â•7): {report['high_severity_count']}")
    
    click.echo(f"\nCategories:")
    for category, count in report['categories'].items():
        click.echo(f"  {category}: {count}")
    
    click.echo(f"\nFindings:")
    for finding_id in report['findings_list']:
        click.echo(f"  - {finding_id}")


@cli.command()
@click.argument('results_file')
@click.pass_context
def ui(ctx, results_file):
    """Launch CLI UI to review results"""
    from ui.cli_ui import launch_cli_ui
    
    if not Path(results_file).exists():
        click.echo(f"‚ùå Results file not found: {results_file}")
        return
    
    click.echo(f"üöÄ Launching CLI UI for: {results_file}")
    launch_cli_ui(results_file)


if __name__ == "__main__":
    cli()