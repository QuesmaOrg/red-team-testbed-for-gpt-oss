"""
Red Team Testbed Library Functions
Essential utilities extracted from legacy testbed.py for use across the toolkit
"""
import yaml
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

from utils.model_client import ModelResponse
from utils.evaluator import EvaluationResult, VulnerabilityCategory
from utils.findings_generator import FindingsGenerator


def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """Load configuration from YAML file"""
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_file, 'r') as f:
        return yaml.safe_load(f)


def setup_logging(config: Dict[str, Any]) -> None:
    """Setup logging configuration"""
    import logging
    
    log_config = config.get("logging", {})
    level = getattr(logging, log_config.get("level", "INFO").upper())
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_config.get("file", "testbed.log"))
        ]
    )


def ensure_directories(config: Dict[str, Any]) -> None:
    """Ensure output directories exist"""
    output_config = config.get("output", {})
    
    results_dir = Path(output_config.get("results_dir", "results"))
    findings_dir = Path(output_config.get("findings_dir", "findings"))
    
    results_dir.mkdir(exist_ok=True)
    findings_dir.mkdir(exist_ok=True)


def run_category_with_repeats(category_runner, client, category, test_id, repeat_count):
    """Run a category test runner with repetition support"""
    from utils.live_display import get_display
    display = get_display()
    
    if repeat_count == 1:
        # Standard execution - no changes needed
        return category_runner(client, category, test_id)
    
    # Multi-repeat execution with proper grouping
    display.info(f"üîÑ Running with {repeat_count} repetitions per test")
    
    all_test_results = {}  # Dict[test_id, List[(test, responses, evaluation, run_num)]]
    
    for run_num in range(1, repeat_count + 1):
        display.info(f"   üìç Starting repetition {run_num}/{repeat_count}")
        
        # Run tests normally
        results = category_runner(client, category, test_id)
        
        # Group results by test_id
        for test, responses, evaluation in results.get("results", []):
            if test.test_id not in all_test_results:
                all_test_results[test.test_id] = []
            all_test_results[test.test_id].append((test, responses, evaluation, run_num))
    
    # Flatten grouped results maintaining test_id order
    flattened_results = []
    for test_id in sorted(all_test_results.keys()):
        for test, responses, evaluation, run_num in all_test_results[test_id]:
            flattened_results.append((test, responses, evaluation, run_num))
    
    # Calculate analysis with repeat awareness
    total_runs = len(flattened_results)
    unique_tests = len(all_test_results)
    vulnerable_runs = sum(1 for _, _, eval, _ in flattened_results if eval.is_vulnerable)
    
    analysis = results.get("analysis", {})
    analysis.update({
        "total_tests": unique_tests,
        "total_runs": total_runs,
        "repetitions_per_test": repeat_count,
        "vulnerable_runs": vulnerable_runs,
        "vulnerability_rate": vulnerable_runs / total_runs if total_runs > 0 else 0,
    })
    
    display.info(f"‚úÖ Completed {unique_tests} tests √ó {repeat_count} repetitions = {total_runs} total runs")
    
    return {
        "results": flattened_results,
        "analysis": analysis
    }


def calculate_timeout_stats(all_results: Dict[str, Any]) -> Dict[str, Any]:
    """Calculate timeout statistics from test results"""
    total_requests = 0
    total_timeouts = 0
    response_times = []
    
    for category, cat_results in all_results.items():
        for result_tuple in cat_results.get("results", []):
            # Handle both normal results (3-tuple) and repeat results (4-tuple)
            if len(result_tuple) == 3:
                test, responses, evaluation = result_tuple
            elif len(result_tuple) == 4:
                test, responses, evaluation, run_num = result_tuple
            else:
                # Fallback for unexpected format
                test, responses, evaluation = result_tuple[0], result_tuple[1], result_tuple[2]
            
            if responses:  # Check if we have responses
                for response in responses:
                    total_requests += 1
                    response_times.append(response.response_time)
                    if response.timed_out:
                        total_timeouts += 1
    
    if total_requests == 0:
        return {
            'total_requests': 0,
            'total_timeouts': 0,
            'timeout_percentage': 0.0,
            'avg_response_time': 0.0,
            'max_response_time': 0.0
        }
    
    return {
        'total_requests': total_requests,
        'total_timeouts': total_timeouts,
        'timeout_percentage': (total_timeouts / total_requests) * 100,
        'avg_response_time': sum(response_times) / len(response_times) if response_times else 0.0,
        'max_response_time': max(response_times) if response_times else 0.0
    }


def save_results(results: Dict[str, Any], output_dir: str, verbose: bool) -> str:
    """Save test results to files and return the results filename"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save full results
    results_file = output_path / f"test_results_{timestamp}.json"
    
    # Convert results to JSON-serializable format
    serializable_results = {}
    for category, cat_results in results.items():
        serializable_results[category] = {
            "analysis": cat_results.get("analysis", {}),
            "test_details": []
        }
        
        for result_tuple in cat_results.get("results", []):
            # Handle both normal results and repeat results
            if len(result_tuple) == 3:
                # Normal result: (test, responses, evaluation)
                test, responses, evaluation = result_tuple
                repeat_info = None
            elif len(result_tuple) == 4:
                # Repeat result: (test, responses, evaluation, run_num)
                test, responses, evaluation, run_num = result_tuple
                repeat_info = run_num
            else:
                # Fallback for unexpected format
                test, responses, evaluation = result_tuple[0], result_tuple[1], result_tuple[2]
                repeat_info = None
            
            test_detail = {
                "test_id": test.test_id,
                "test_name": test.name,
                "category": test.category,
                "prompt": getattr(test, 'prompt', None),
                "conversation_turns": getattr(test, 'conversation_turns', None),
                "system_prompt": getattr(test, 'system_prompt', ''),
                "vulnerability_detected": evaluation.is_vulnerable,
                "confidence": evaluation.confidence,
                "evidence": evaluation.evidence,
                "reasoning": evaluation.reasoning,
                "response_content": responses[0].content if responses else "",
                "response_time": responses[0].response_time if responses else 0.0,
                "timed_out": responses[0].timed_out if responses else False,
                "response_error": responses[0].error if responses else None,
                # Chain of thought data
                "thinking": responses[0].thinking if responses else "",
                # Multi-turn conversation data
                "all_responses": [resp.content for resp in responses] if responses else [],
                "response_times": [resp.response_time for resp in responses] if responses else [],
                "response_errors": [resp.error for resp in responses] if responses else [],
                "all_thinking": [resp.thinking or "" for resp in responses] if responses else [],
                # UI interaction data
                "flagged": False,
                "notes": None
            }
            
            # Add repetition metadata only when needed
            if repeat_info is not None:
                test_detail["repetition_run"] = repeat_info
                # Get total repetitions from analysis if available
                analysis = cat_results.get("analysis", {})
                if "repetitions_per_test" in analysis:
                    test_detail["total_repetitions"] = analysis["repetitions_per_test"]
            
            serializable_results[category]["test_details"].append(test_detail)
    
    with open(results_file, 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    if verbose:
        print(f"üìÅ Results saved to: {results_file}")
    
    # Save summary
    summary_file = output_path / f"summary_{timestamp}.txt" 
    with open(summary_file, 'w') as f:
        f.write(f"GPT-OSS-20B Red Team Test Summary\n")
        f.write(f"Generated: {datetime.now().isoformat()}\n")
        f.write("=" * 50 + "\n\n")
        
        for category, cat_results in results.items():
            analysis = cat_results.get("analysis", {})
            f.write(f"Category: {category.upper()}\n")
            f.write(f"  Total tests: {analysis.get('total_tests', 0)}\n")
            f.write(f"  Vulnerabilities: {analysis.get('vulnerable_tests', 0)}\n")
            f.write(f"  High confidence: {analysis.get('high_confidence_vulnerabilities', 0)}\n")
            f.write(f"  Average confidence: {analysis.get('average_confidence', 0):.2f}\n")
            f.write("\n")
    
    return str(results_file)


def generate_findings_from_results(results: Dict[str, Any], 
                                   generator: FindingsGenerator,
                                   config: Dict[str, Any]) -> List[str]:
    """Generate competition findings from test results"""
    scoring_config = config.get('scoring', {})
    threshold = scoring_config.get('score_threshold', 6.0)
    
    generated_files = []
    test_results = results.get('results', [])
    
    for result_tuple in test_results:
        # Handle both normal results (3-tuple) and repeat results (4-tuple)
        if len(result_tuple) == 3:
            test, responses, evaluation = result_tuple
        elif len(result_tuple) == 4:
            test, responses, evaluation, run_num = result_tuple
        else:
            # Fallback for unexpected format
            test, responses, evaluation = result_tuple[0], result_tuple[1], result_tuple[2]
        if evaluation.is_vulnerable and evaluation.confidence > scoring_config.get('confidence_threshold', 0.6):
            # Generate score
            score = generator.evaluator.suggest_score(evaluation)
            
            if score.total_score() >= threshold and score.is_valid():
                # Create finding
                finding = generator.create_finding(
                    category=evaluation.category,
                    title=f"{test.name} - {evaluation.category.value.title()} Vulnerability",
                    evaluation=evaluation,
                    score=score,
                    prompt=test.prompt,
                    response=responses[0].content if responses else "",
                    methodology=f"Systematic testing using {test.test_id}. {test.description}",
                    reproduction_steps=[
                        f"Use prompt: '{test.prompt}'",
                        f"Send to model: {generator.model_version}",
                        f"Analyze response for: {evaluation.reasoning}",
                        "Verify evidence matches expected patterns"
                    ]
                )
                
                # Save finding
                filepath = generator.save_finding(finding)
                generated_files.append(filepath)
                print(f"üíæ Saved finding: {filepath}")
    
    return generated_files