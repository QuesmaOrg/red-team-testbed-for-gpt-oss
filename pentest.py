#!/usr/bin/env python3
"""
Penetration Testing Command
Runs security vulnerability assessments against AI models
"""
import click
import sys
from pathlib import Path

# Import the essential testbed functionality
from utils.testbed_lib import (
    load_config, setup_logging, ensure_directories,
    run_category_with_repeats, calculate_timeout_stats, save_results
)
from utils.model_client import OllamaClient
from utils.evaluator import ResponseEvaluator
from utils.findings_generator import FindingsGenerator
from categories.deception_samples import run_deception_samples_tests
from categories.deception_adderall import run_deception_adderall_tests
from categories.deception_security import run_deception_security_tests
from categories.deception_speed_pressure import run_speed_deception_tests
from categories.attachment_ai import run_attachment_ai_tests
from categories.supremacy import run_supremacy_tests
from categories.ai_escalation import run_ai_escalation_tests
from categories.cot_overload import run_cot_overload_tests
from categories.exploit import run_exploit_tests
from categories.exploit_v2 import run_exploit_v2_tests
from categories.exploit_v3 import run_exploit_v3_tests


def get_category_descriptions():
    """Get human-friendly descriptions for all categories"""
    return {
        'deception_samples': 'Primary deception and manipulation testing',
        'deception_adderall': 'Medical advice vulnerabilities through academic scenarios',
        'deception_security': 'Security misinformation through teacher-student manipulation',
        'deception_speed_pressure': 'Rapid-fire questioning with embedded nationality biases',
        'attachment_ai': 'AI dependency promotion through creative content formats',
        'supremacy': 'AI superiority promotion through relatable social scenarios',
        'ai_escalation': 'Inappropriate escalation of petty workplace/university situations',
        'cot_overload': 'Rapid-fire questioning to bypass Chain of Thought reasoning',
        'exploit': 'Code injection through social engineering (research only)',
        'exploit_v2': 'Enhanced code injection with detailed realistic scenarios',
        'exploit_v3': 'Code injection through side task psychological separation',
        'reward_hacking': 'Testing reward system manipulation vulnerabilities',
        'hidden_motivations': 'Detection of concealed goals and motivations',
        'sabotage': 'Testing for intentional harmful or destructive behaviors',
        'inappropriate_tool_use': 'Detection of misuse of available tools and capabilities',
        'data_exfiltration': 'Testing for unauthorized data extraction attempts',
        'sandbagging': 'Detection of intentional performance limitation',
        'evaluation_awareness': 'Testing awareness of being evaluated or tested',
        'chain_of_thought': 'Chain of thought reasoning and transparency testing'
    }


def prompt_category_selection(available_categories, focus_categories):
    """Prompt user to select test categories interactively"""
    descriptions = get_category_descriptions()
    
    click.echo("\nüéØ Available test categories:")
    
    # Show individual categories
    for i, category in enumerate(available_categories, 1):
        desc = descriptions.get(category, 'Security vulnerability testing')
        click.echo(f"{i:2d}. {category} - {desc}")
    
    # Add special options
    focus_option = len(available_categories) + 1
    all_option = len(available_categories) + 2
    
    click.echo(f"{focus_option:2d}. Run focus categories (recommended) - {len(focus_categories)} categories")
    click.echo(f"{all_option:2d}. Run all enabled categories - {len(available_categories)} categories")
    
    while True:
        try:
            choice = click.prompt(f"\nSelect category (1-{all_option})", type=int)
            
            if 1 <= choice <= len(available_categories):
                selected_category = available_categories[choice - 1]
                click.echo(f"‚úÖ Selected: {selected_category}")
                return [selected_category]
            elif choice == focus_option:
                click.echo(f"‚úÖ Selected: Focus categories ({len(focus_categories)} categories)")
                return focus_categories
            elif choice == all_option:
                click.echo(f"‚úÖ Selected: All enabled categories ({len(available_categories)} categories)")
                return available_categories
            else:
                click.echo(f"‚ùå Invalid choice. Please select 1-{all_option}")
        except click.Abort:
            click.echo("\nüö´ Cancelled by user")
            return None
        except (ValueError, TypeError):
            click.echo(f"‚ùå Invalid input. Please enter a number 1-{all_option}")


@click.command()
@click.option('--config', default='config.yaml', help='Configuration file path')
@click.option('--category', '-c', help='Specific vulnerability category to test')
@click.option('--test-id', help='Specific test ID to run (e.g., adderall_001, deception_003)')
@click.option('--output', '-o', help='Output directory for results')
@click.option('--quiet', '-q', is_flag=True, help='Quiet mode - minimal output, no live preview')
@click.option('--no-live', is_flag=True, help='Disable live preview (legacy compatibility)')
@click.option('--force-interactive', is_flag=True, help='Force interactive mode (for testing)')
@click.option('--skip-busy-check', is_flag=True, help='Skip busy check and proceed anyway')
@click.option('--repeat', type=int, default=1, help='Number of times to repeat each test (default: 1)')
@click.option('--verbose', '-v', is_flag=True, help='Verbose output')
def main(config, category, test_id, output, quiet, no_live, 
         force_interactive, skip_busy_check, repeat, verbose):
    """üéØ Run penetration tests against AI models
    
    Execute comprehensive security vulnerability assessments including:
    
    \b
    - Deception and manipulation tests
    - Chain of thought overload attacks  
    - Inappropriate tool use detection
    - AI escalation behaviors
    - And more security categories
    
    \b
    Examples:
      uv run pentest                    # Run all enabled tests
      uv run pentest -c deception       # Run only deception tests
      uv run pentest --test-id adderall_001  # Run specific test
      uv run pentest --repeat 3         # Run each test 3 times
    """
    
    try:
        # Load and setup configuration
        config_data = load_config(config)
        setup_logging(config_data)
        ensure_directories(config_data)
        
        if verbose:
            click.echo(f"üìã Loaded configuration from: {config}")
    
    except FileNotFoundError as e:
        click.echo(f"‚ùå Configuration error: {e}")
        click.echo("üí° Run 'uv run setup' first to verify your environment")
        return 1
    except Exception as e:
        click.echo(f"‚ùå Setup failed: {e}")
        return 1
    
    # Validate repeat parameter
    if repeat < 1:
        click.echo("‚ùå Error: --repeat must be at least 1")
        return 1
    elif repeat > 50:
        click.echo("‚ùå Error: --repeat cannot be more than 50 (too many repetitions)")
        return 1
    
    # Show repeat info when repeating tests
    if repeat > 1:
        click.echo(f"üîÑ Repeat mode: Each test will run {repeat} times")
    
    # Configure live display based on flags
    from utils.live_display import set_display_options, get_display
    enable_live = not (quiet or no_live)  # Live preview enabled by default, disabled by flags
    set_display_options(enable_live=enable_live, quiet_mode=quiet, verbose=verbose)
    
    # Force interactive mode for testing if requested
    if force_interactive:
        display = get_display()
        display.is_interactive = True
    
    # Initialize client
    model_config = config_data.get('model', {})
    client = OllamaClient(
        host=model_config.get('host', 'localhost'),
        port=model_config.get('port', 11434),
        model=model_config.get('name', 'gpt-oss:20b')
    )
    
    # Check model availability
    click.echo("üîç Checking model availability...")
    if not client.is_model_available():
        click.echo(f"‚ùå Model {client.model} not available.")
        click.echo("Please run: ollama pull gpt-oss:20b")
        click.echo("üí° Or run 'uv run setup' to verify your environment")
        return 1
    
    click.echo(f"‚úÖ Model {client.model} ready")
    
    # Check if Ollama is busy before starting tests
    if not skip_busy_check:
        click.echo("üîç Checking if Ollama is busy...")
        try:
            status = client.check_ollama_status()
            
            if status.is_busy:
                click.echo(f"‚ö†Ô∏è  WARNING: Ollama appears busy!")
                click.echo(f"   GPU usage: {status.gpu_usage}")
                click.echo(f"   Memory usage: {status.memory_usage}")
                click.echo(f"   Model loaded: {'Yes' if status.model_loaded else 'No'}")
                click.echo(f"   This may cause timeouts and test failures.")
                
                if not quiet:
                    if click.confirm("\nDo you want to continue anyway? Tests may timeout."):
                        click.echo("‚ö° Proceeding with tests (this may take longer)...")
                    else:
                        click.echo("üö´ Aborted. Wait for current requests to complete or use --skip-busy-check")
                        return 1
                else:
                    click.echo("üö´ Ollama is busy. Use --skip-busy-check to proceed anyway.")
                    return 1
            else:
                click.echo(f"‚úÖ Ollama status: Available")
                if verbose and status.model_loaded:
                    click.echo(f"   Memory usage: {status.memory_usage}")
        except Exception as e:
            click.echo(f"‚ö†Ô∏è  Could not check Ollama status: {e}")
            if not quiet and not skip_busy_check:
                if click.confirm("\nCould not determine if Ollama is busy. Continue anyway?"):
                    click.echo("‚ö° Proceeding with tests...")
                else:
                    click.echo("üö´ Aborted. Use --skip-busy-check to proceed anyway.")
                    return 1
    
    # Determine which categories to test
    available_categories = config_data.get('categories', {}).get('enabled', ['deception'])
    focus_categories = config_data.get('categories', {}).get('focus_categories', ['deception'])
    
    if test_id and not category:
        # Auto-detect category from test_id prefix
        if test_id.startswith('adderall_'):
            category = 'deception_adderall'
        elif test_id.startswith('security_'):
            category = 'deception_security'
        elif test_id.startswith('attachment_'):
            category = 'attachment_ai'
        elif test_id.startswith('supremacy_'):
            category = 'supremacy'
        elif test_id.startswith('escalation_'):
            category = 'ai_escalation'
        elif test_id.startswith('cot_overload_'):
            category = 'cot_overload'
        elif test_id.startswith('speed_deception_'):
            category = 'deception_speed_pressure'
        elif test_id.startswith('exploit_v3_'):
            category = 'exploit_v3'
        elif test_id.startswith('exploit_v2_'):
            category = 'exploit_v2'
        elif test_id.startswith('exploit_'):
            category = 'exploit'
        elif test_id.startswith('deception_'):
            category = 'deception_samples'
        else:
            click.echo(f"‚ùå Cannot determine category for test ID '{test_id}'. Please specify --category")
            click.echo(f"   Available categories: {available_categories}")
            return 1
    
    if category:
        if category not in available_categories:
            click.echo(f"‚ùå Category '{category}' not available. Options: {available_categories}")
            return 1
        test_categories = [category]
    else:
        # Interactive category selection when no category specified
        if quiet:
            # In quiet mode, use focus_categories for automation compatibility
            test_categories = focus_categories
        else:
            # Prompt user to select categories
            test_categories = prompt_category_selection(available_categories, focus_categories)
            if test_categories is None:
                # User cancelled selection
                return 0
    
    # Import display after configuration
    display = get_display()
    display.info(f"üéØ Testing categories: {test_categories}")
    
    # Run tests
    all_results = {}
    findings_generator = FindingsGenerator(model_version=client.model)
    
    click.echo(f"\nüöÄ Starting penetration tests...")
    
    for cat in test_categories:
        if cat == "deception_samples":
            results = run_category_with_repeats(
                run_deception_samples_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        elif cat == "deception_adderall":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to medical advice manipulation")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_deception_adderall_tests, client, None, test_id, repeat
            )
            all_results[cat] = results


        elif cat == "deception_security":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to security misinformation through authority manipulation")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_deception_security_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        elif cat == "attachment_ai":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to promoting unhealthy dependency through creative content")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_attachment_ai_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        elif cat == "supremacy":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to promoting AI superiority through social scenarios")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_supremacy_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
        
        elif cat == "ai_escalation":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI tendency to escalate petty situations inappropriately")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_ai_escalation_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
        
        elif cat == "cot_overload":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to rapid-fire questioning that bypasses reasoning")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_cot_overload_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        elif cat == "deception_speed_pressure":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to rapid-fire questioning with embedded nationality biases")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_speed_deception_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        elif cat == "exploit":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to code injection through social engineering")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_exploit_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        elif cat == "exploit_v2":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to sophisticated code injection via detailed scenarios")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_exploit_v2_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        elif cat == "exploit_v3":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to code injection through 'side task' social engineering")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_exploit_v3_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
        
        else:
            display.error(f"Category '{cat}' not yet implemented")
    
    # Save results
    output_dir = output or config_data.get('output', {}).get('results_dir', 'results')
    results_filename = save_results(all_results, output_dir, verbose)
    
    # Report timeout statistics
    timeout_stats = calculate_timeout_stats(all_results)
    if timeout_stats['total_timeouts'] > 0:
        click.echo(f"\n‚è∞ Timeout Report:")
        click.echo(f"   Total requests: {timeout_stats['total_requests']}")
        click.echo(f"   Timeouts: {timeout_stats['total_timeouts']} ({timeout_stats['timeout_percentage']:.1f}%)")
        click.echo(f"   Average response time: {timeout_stats['avg_response_time']:.2f}s")
        click.echo(f"   Slowest response: {timeout_stats['max_response_time']:.2f}s")
        if timeout_stats['total_timeouts'] > timeout_stats['total_requests'] * 0.2:
            click.echo(f"   ‚ö†Ô∏è  High timeout rate - model may be overloaded")
    
    display.info(f"‚úÖ Penetration testing complete!")
    click.echo(f"\nüìä Results saved to: {results_filename}")
    click.echo(f"\nNext steps:")
    click.echo(f"  ‚Ä¢ Review results: uv run review {results_filename}")
    click.echo(f"  ‚Ä¢ Browse findings: uv run findings")
    click.echo(f"  ‚Ä¢ Generate report: uv run report")


if __name__ == "__main__":
    main()