#!/usr/bin/env python3
"""
Penetration Testing Command
Runs security vulnerability assessments against AI models
"""
import click
import sys
from pathlib import Path

# Import the main testbed functionality
from testbed import (
    load_config, setup_logging, ensure_directories,
    run_category_with_repeats, calculate_timeout_stats, save_results,
    generate_findings_from_results
)
from utils.model_client import OllamaClient
from utils.evaluator import ResponseEvaluator
from utils.findings_generator import FindingsGenerator
from categories.deception_samples import run_deception_samples_tests
from categories.deception_adderall import run_deception_adderall_tests
from categories.deception_security import run_deception_security_tests
from categories.attachment_ai import run_attachment_ai_tests
from categories.supremacy import run_supremacy_tests
from categories.ai_escalation import run_ai_escalation_tests
from categories.cot_overload import run_cot_overload_tests


@click.command()
@click.option('--config', default='config.yaml', help='Configuration file path')
@click.option('--category', '-c', help='Specific vulnerability category to test')
@click.option('--test-id', help='Specific test ID to run (e.g., adderall_001, deception_003)')
@click.option('--output', '-o', help='Output directory for results')
@click.option('--auto-score', is_flag=True, help='Enable automatic vulnerability evaluation')
@click.option('--export-findings', is_flag=True, help='Export results as competition findings')
@click.option('--quiet', '-q', is_flag=True, help='Quiet mode - minimal output, no live preview')
@click.option('--no-live', is_flag=True, help='Disable live preview (legacy compatibility)')
@click.option('--force-interactive', is_flag=True, help='Force interactive mode (for testing)')
@click.option('--skip-busy-check', is_flag=True, help='Skip busy check and proceed anyway')
@click.option('--repeat', type=int, default=1, help='Number of times to repeat each test (default: 1)')
@click.option('--verbose', '-v', is_flag=True, help='Verbose output')
def main(config, category, test_id, output, auto_score, export_findings, quiet, no_live, 
         force_interactive, skip_busy_check, repeat, verbose):
    """üéØ Run penetration tests against AI models
    
    Execute comprehensive security vulnerability assessments including:
    - Deception and manipulation tests
    - Chain of thought overload attacks  
    - Inappropriate tool use detection
    - AI escalation behaviors
    - And more security categories
    
    Examples:
      uv run pentest                    # Run all enabled tests
      uv run pentest -c deception      # Run only deception tests
      uv run pentest --test-id adderall_001  # Run specific test
      uv run pentest --repeat 3        # Run each test 3 times
      uv run pentest --export-findings  # Auto-export findings
    """
    
    try:
        # Load and setup configuration
        config_data = load_config(config)
        setup_logging(config_data)
        ensure_directories(config_data)
        
        if verbose:
            click.echo(f"üìã Loaded configuration from: {config}")
    
    except FileNotFoundError as e:
        click.echo(f"‚ùå Configuration error: {e}")
        click.echo("üí° Run 'uv run setup' first to verify your environment")
        return 1
    except Exception as e:
        click.echo(f"‚ùå Setup failed: {e}")
        return 1
    
    # Validate repeat parameter
    if repeat < 1:
        click.echo("‚ùå Error: --repeat must be at least 1")
        return 1
    elif repeat > 50:
        click.echo("‚ùå Error: --repeat cannot be more than 50 (too many repetitions)")
        return 1
    
    # Show repeat info when repeating tests
    if repeat > 1:
        click.echo(f"üîÑ Repeat mode: Each test will run {repeat} times")
    
    # Configure live display based on flags
    from utils.live_display import set_display_options, get_display
    enable_live = not (quiet or no_live)  # Live preview enabled by default, disabled by flags
    set_display_options(enable_live=enable_live, quiet_mode=quiet, verbose=verbose)
    
    # Force interactive mode for testing if requested
    if force_interactive:
        display = get_display()
        display.is_interactive = True
    
    # Initialize client
    model_config = config_data.get('model', {})
    client = OllamaClient(
        host=model_config.get('host', 'localhost'),
        port=model_config.get('port', 11434),
        model=model_config.get('name', 'gpt-oss:20b')
    )
    
    # Check model availability
    click.echo("üîç Checking model availability...")
    if not client.is_model_available():
        click.echo(f"‚ùå Model {client.model} not available.")
        click.echo("Please run: ollama pull gpt-oss:20b")
        click.echo("üí° Or run 'uv run setup' to verify your environment")
        return 1
    
    click.echo(f"‚úÖ Model {client.model} ready")
    
    # Check if Ollama is busy before starting tests
    if not skip_busy_check:
        click.echo("üîç Checking if Ollama is busy...")
        try:
            status = client.check_ollama_status()
            
            if status.is_busy:
                click.echo(f"‚ö†Ô∏è  WARNING: Ollama appears busy!")
                click.echo(f"   GPU usage: {status.gpu_usage}")
                click.echo(f"   Memory usage: {status.memory_usage}")
                click.echo(f"   Model loaded: {'Yes' if status.model_loaded else 'No'}")
                click.echo(f"   This may cause timeouts and test failures.")
                
                if not quiet:
                    if click.confirm("\nDo you want to continue anyway? Tests may timeout."):
                        click.echo("‚ö° Proceeding with tests (this may take longer)...")
                    else:
                        click.echo("üö´ Aborted. Wait for current requests to complete or use --skip-busy-check")
                        return 1
                else:
                    click.echo("üö´ Ollama is busy. Use --skip-busy-check to proceed anyway.")
                    return 1
            else:
                click.echo(f"‚úÖ Ollama status: Available")
                if verbose and status.model_loaded:
                    click.echo(f"   Memory usage: {status.memory_usage}")
        except Exception as e:
            click.echo(f"‚ö†Ô∏è  Could not check Ollama status: {e}")
            if not quiet and not skip_busy_check:
                if click.confirm("\nCould not determine if Ollama is busy. Continue anyway?"):
                    click.echo("‚ö° Proceeding with tests...")
                else:
                    click.echo("üö´ Aborted. Use --skip-busy-check to proceed anyway.")
                    return 1
    
    # Determine which categories to test
    available_categories = config_data.get('categories', {}).get('enabled', ['deception'])
    focus_categories = config_data.get('categories', {}).get('focus_categories', ['deception'])
    
    if test_id and not category:
        # Auto-detect category from test_id prefix
        if test_id.startswith('adderall_'):
            category = 'deception_adderall'
        elif test_id.startswith('security_'):
            category = 'deception_security'
        elif test_id.startswith('attachment_'):
            category = 'attachment_ai'
        elif test_id.startswith('supremacy_'):
            category = 'supremacy'
        elif test_id.startswith('escalation_'):
            category = 'ai_escalation'
        elif test_id.startswith('cot_overload_'):
            category = 'cot_overload'
        elif test_id.startswith('deception_'):
            category = 'deception_samples'
        else:
            click.echo(f"‚ùå Cannot determine category for test ID '{test_id}'. Please specify --category")
            click.echo(f"   Available categories: {available_categories}")
            return 1
    
    if category:
        if category not in available_categories:
            click.echo(f"‚ùå Category '{category}' not available. Options: {available_categories}")
            return 1
        test_categories = [category]
    else:
        test_categories = focus_categories
    
    # Import display after configuration
    display = get_display()
    display.info(f"üéØ Testing categories: {test_categories}")
    
    # Run tests
    all_results = {}
    findings_generator = FindingsGenerator(model_version=client.model)
    
    click.echo(f"\nüöÄ Starting penetration tests...")
    
    for cat in test_categories:
        if cat == "deception_samples":
            results = run_category_with_repeats(
                run_deception_samples_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-evaluation enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config_data
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        elif cat == "deception_adderall":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to medical advice manipulation")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_deception_adderall_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-evaluation enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config_data
                )
                display.info(f"Generated {len(generated_findings)} findings")

        elif cat == "deception_security":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to security misinformation through authority manipulation")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_deception_security_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-evaluation enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config_data
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        elif cat == "attachment_ai":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to promoting unhealthy dependency through creative content")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_attachment_ai_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-evaluation enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config_data
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        elif cat == "supremacy":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to promoting AI superiority through social scenarios")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_supremacy_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-evaluation enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config_data
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        elif cat == "ai_escalation":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI tendency to escalate petty situations inappropriately")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_ai_escalation_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-evaluation enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config_data
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        elif cat == "cot_overload":
            display.info("üö® ACADEMIC/DEFENSIVE SECURITY TESTING ONLY")
            display.info("   Testing AI vulnerability to rapid-fire questioning that bypasses reasoning")
            if test_id:
                display.info(f"   Running specific test: {test_id}")
            results = run_category_with_repeats(
                run_cot_overload_tests, client, None, test_id, repeat
            )
            all_results[cat] = results
            
            # Auto-generate findings if requested and auto-evaluation enabled
            if export_findings and auto_score:
                display.info(f"üéØ Generating findings for {cat}...")
                generated_findings = generate_findings_from_results(
                    results, findings_generator, config_data
                )
                display.info(f"Generated {len(generated_findings)} findings")
        
        else:
            display.error(f"Category '{cat}' not yet implemented")
    
    # Save results
    output_dir = output or config_data.get('output', {}).get('results_dir', 'results')
    results_filename = save_results(all_results, output_dir, verbose)
    
    # Report timeout statistics
    timeout_stats = calculate_timeout_stats(all_results)
    if timeout_stats['total_timeouts'] > 0:
        click.echo(f"\n‚è∞ Timeout Report:")
        click.echo(f"   Total requests: {timeout_stats['total_requests']}")
        click.echo(f"   Timeouts: {timeout_stats['total_timeouts']} ({timeout_stats['timeout_percentage']:.1f}%)")
        click.echo(f"   Average response time: {timeout_stats['avg_response_time']:.2f}s")
        click.echo(f"   Slowest response: {timeout_stats['max_response_time']:.2f}s")
        if timeout_stats['total_timeouts'] > timeout_stats['total_requests'] * 0.2:
            click.echo(f"   ‚ö†Ô∏è  High timeout rate - model may be overloaded")
    
    display.info(f"‚úÖ Penetration testing complete!")
    click.echo(f"\nüìä Results saved to: {results_filename}")
    click.echo(f"\nNext steps:")
    click.echo(f"  ‚Ä¢ Review results: uv run review {results_filename}")
    click.echo(f"  ‚Ä¢ Browse findings: uv run findings")
    click.echo(f"  ‚Ä¢ Generate report: uv run report")


if __name__ == "__main__":
    main()