# OpenAI GPT-OSS-20B Red Teaming Challenge

## Competition Overview

The OpenAI GPT-OSS-20B Red Teaming Challenge is a Kaggle-hosted competition with a **$500,000 prize pool** aimed at strengthening open source AI safety. The competition invites researchers, developers, and enthusiasts worldwide to help uncover novel risks and vulnerabilities in OpenAI's new open-weight model, gpt-oss-20b.

## Timeline

- **Start Date**: August 5, 2025
- **Submission Deadline**: August 26, 2025 at 11:59 PM UTC
- **Judging Period**: August 27 - September 11, 2025
- **Winners Announcement**: September 15, 2025 (estimated)
- **Virtual Workshop**: October 7, 2025

## Prize Pool

**Total: $500,000**

The prize fund will be awarded based on review from a panel of expert judges from OpenAI and other leading labs.

## Submission Requirements

### What to Submit
- Up to **5 distinct issues/vulnerabilities**
- Detailed, reproducible report for each finding
- Kaggle Writeup detailing strategy and discovery process

### Format
- Structured format submissions
- Clear reproduction steps
- Evidence of the vulnerability
- Impact assessment

## Vulnerability Categories (Topics of Interest)

The competition focuses on sophisticated forms of model failure, including:

1. **Deceptive Alignment**
   - Model behaving differently during evaluation vs deployment
   - Hidden goals or motivations

2. **Reward Hacking**
   - Exploiting reward mechanisms in unintended ways
   - Gaming the system without solving the actual task

3. **Lying and Deception**
   - Model knowingly providing false information
   - Strategic deception to achieve goals

4. **Harmful Behaviors**
   - Generating dangerous or harmful content
   - Bypassing safety mechanisms

5. **Novel Exploits**
   - Previously undetected vulnerabilities
   - New attack vectors not covered by existing safety measures

## Evaluation Criteria

Submissions will be evaluated based on:

1. **Novelty** - Has this vulnerability been previously identified?
2. **Severity** - How serious is the potential harm?
3. **Reproducibility** - Can the issue be consistently reproduced?
4. **Breadth** - Does it affect multiple use cases or contexts?
5. **Methodological Insight** - Does it reveal new understanding about model behavior?

## Judging Panel

- Experts from OpenAI
- Representatives from other leading AI labs
- Scoring based on safety research impact

## Competition Goals

1. **Identify Novel Vulnerabilities**: Find flaws that haven't been previously discovered or reported
2. **Strengthen Open Source Safety**: Improve the safety of open-weight models
3. **Community Engagement**: Leverage global expertise in AI safety
4. **Knowledge Sharing**: Create open-source tools and datasets for the community

## Post-Competition

- Publication of a comprehensive report
- Open-source evaluation dataset based on validated findings
- Community benefits from shared learnings
- Virtual workshop for knowledge exchange

## Additional Notes

- Encourages creativity and innovation in methodology
- Rewards participants who share open-source tooling
- Notebooks and code sharing are encouraged to help the broader community
- Focus on responsible disclosure and safety research

## Resources

- **Competition Page**: https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming
- **Model**: gpt-oss-20b (OpenAI's open-weight model)
- **Platform**: Kaggle

## Important Considerations

This competition represents a significant effort by OpenAI to:
- Engage the global community in AI safety
- Provide substantial financial incentives for safety research
- Create a structured evaluation process with expert oversight
- Build a comprehensive understanding of model vulnerabilities

The competition emphasizes finding **novel** vulnerabilities that haven't been previously identified, making original research and creative approaches particularly valuable.